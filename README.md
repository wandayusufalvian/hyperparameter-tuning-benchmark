# hyperparameter-tuning-benchmark
Comparation of BOHB as hyperparameter optimization method with random search, grid search, and bayesian optimization to tune GBDT algorithm using 3 different dataset 

Requirements : 
- python 3.7 
- scikit-learn  0.24 
- numpy 1.16.2 
- pandas 0.24.2 
- configspace 0.4.17
- scikit-optimize 0.8.1

Hyperparameter Optimization : 
- grid search
- random search
- bayesian optimization 
- BOHB (http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf) 

GBDT Algorithms : 
- XGBoost 
- LightGBM
- CatBoost 

Dataset : 
- [Bank Marketing Dataset](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)
- [Default Of Credit Card Clients Dataset](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)
- [Census Income Dataset](http://archive.ics.uci.edu/ml/datasets/Census+Income)

Experiments :
- save all code in google drive that sync with google drive app in my local machine 
- open and run code in google drive using google colab 

