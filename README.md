# hyperparameter-tuning-benchmark

## Purpose of research 

Optimize hyperparameter of 3 most popular GBDT Algorithms : XGBoost, LightGBM, and CatBoost using BOHB, state of the art Hyperparameter Optimization (HPO) algorithm. For comparison to BOHB performance, i use 3 others HPO algorithm : Grid Search, Random Search, and Bayesian Optimization.

## What is BOHB? 

BOHB is state of the art Hyperparameter Optimization algorithm that was  developed by Falkner, et.al (2018) : http://proceedings.mlr.press/v80/falkner18a.html

## Dataset : 

3 kind of dataset : imbalance binary class, multiclass, and numerical dataset. 

## Environment : 
- python 3.8.3
- anaconda 1.4.0
- Linux Ubuntu 18.04 
- 

